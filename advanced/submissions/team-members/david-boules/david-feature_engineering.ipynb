{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8da6bf27",
   "metadata": {},
   "source": [
    "# Phase 2 - Feature Engineering & Deep Learning Prep."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98b639",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c6ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d5425",
   "metadata": {},
   "source": [
    "### Loading 'cleaned' data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "538be7c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>WS</th>\n",
       "      <th>GDF</th>\n",
       "      <th>DF</th>\n",
       "      <th>PZ1</th>\n",
       "      <th>PZ2</th>\n",
       "      <th>PZ3</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>6.559</td>\n",
       "      <td>73.8</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.119</td>\n",
       "      <td>34055.69620</td>\n",
       "      <td>16128.87538</td>\n",
       "      <td>20240.96386</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:10:00</td>\n",
       "      <td>6.414</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.085</td>\n",
       "      <td>29814.68354</td>\n",
       "      <td>19375.07599</td>\n",
       "      <td>20131.08434</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:20:00</td>\n",
       "      <td>6.313</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.100</td>\n",
       "      <td>29128.10127</td>\n",
       "      <td>19006.68693</td>\n",
       "      <td>19668.43373</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:30:00</td>\n",
       "      <td>6.121</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.096</td>\n",
       "      <td>28228.86076</td>\n",
       "      <td>18361.09422</td>\n",
       "      <td>18899.27711</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:40:00</td>\n",
       "      <td>5.921</td>\n",
       "      <td>75.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.085</td>\n",
       "      <td>27335.69620</td>\n",
       "      <td>17872.34043</td>\n",
       "      <td>18442.40964</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime   temp  humidity     WS    GDF     DF          PZ1  \\\n",
       "0 2017-01-01 00:00:00  6.559      73.8  0.083  0.051  0.119  34055.69620   \n",
       "1 2017-01-01 00:10:00  6.414      74.5  0.083  0.070  0.085  29814.68354   \n",
       "2 2017-01-01 00:20:00  6.313      74.5  0.080  0.062  0.100  29128.10127   \n",
       "3 2017-01-01 00:30:00  6.121      75.0  0.083  0.091  0.096  28228.86076   \n",
       "4 2017-01-01 00:40:00  5.921      75.7  0.081  0.048  0.085  27335.69620   \n",
       "\n",
       "           PZ2          PZ3 day_of_week  hour  month  year  \n",
       "0  16128.87538  20240.96386      Sunday     0      1  2017  \n",
       "1  19375.07599  20131.08434      Sunday     0      1  2017  \n",
       "2  19006.68693  19668.43373      Sunday     0      1  2017  \n",
       "3  18361.09422  18899.27711      Sunday     0      1  2017  \n",
       "4  17872.34043  18442.40964      Sunday     0      1  2017  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"new_data.csv\")\n",
    "df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95235933",
   "metadata": {},
   "source": [
    "### Extracting time-based features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa245d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>WS</th>\n",
       "      <th>GDF</th>\n",
       "      <th>DF</th>\n",
       "      <th>PZ1</th>\n",
       "      <th>PZ2</th>\n",
       "      <th>PZ3</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>6.559</td>\n",
       "      <td>73.8</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.119</td>\n",
       "      <td>34055.69620</td>\n",
       "      <td>16128.87538</td>\n",
       "      <td>20240.96386</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:10:00</td>\n",
       "      <td>6.414</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.085</td>\n",
       "      <td>29814.68354</td>\n",
       "      <td>19375.07599</td>\n",
       "      <td>20131.08434</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:20:00</td>\n",
       "      <td>6.313</td>\n",
       "      <td>74.5</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.100</td>\n",
       "      <td>29128.10127</td>\n",
       "      <td>19006.68693</td>\n",
       "      <td>19668.43373</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:30:00</td>\n",
       "      <td>6.121</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.096</td>\n",
       "      <td>28228.86076</td>\n",
       "      <td>18361.09422</td>\n",
       "      <td>18899.27711</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:40:00</td>\n",
       "      <td>5.921</td>\n",
       "      <td>75.7</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.085</td>\n",
       "      <td>27335.69620</td>\n",
       "      <td>17872.34043</td>\n",
       "      <td>18442.40964</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime   temp  humidity     WS    GDF     DF          PZ1  \\\n",
       "0 2017-01-01 00:00:00  6.559      73.8  0.083  0.051  0.119  34055.69620   \n",
       "1 2017-01-01 00:10:00  6.414      74.5  0.083  0.070  0.085  29814.68354   \n",
       "2 2017-01-01 00:20:00  6.313      74.5  0.080  0.062  0.100  29128.10127   \n",
       "3 2017-01-01 00:30:00  6.121      75.0  0.083  0.091  0.096  28228.86076   \n",
       "4 2017-01-01 00:40:00  5.921      75.7  0.081  0.048  0.085  27335.69620   \n",
       "\n",
       "           PZ2          PZ3  day_of_week  hour  month  year  day_of_year  \n",
       "0  16128.87538  20240.96386            6     0      1  2017            1  \n",
       "1  19375.07599  20131.08434            6     0      1  2017            1  \n",
       "2  19006.68693  19668.43373            6     0      1  2017            1  \n",
       "3  18361.09422  18899.27711            6     0      1  2017            1  \n",
       "4  17872.34043  18442.40964            6     0      1  2017            1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['hour'] = df['datetime'].dt.hour\n",
    "df['day_of_week'] = df['datetime'].dt.dayofweek\n",
    "df['month'] = df['datetime'].dt.month\n",
    "df['day_of_year'] = df['datetime'].dt.dayofyear\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280504f",
   "metadata": {},
   "source": [
    "## Lookback Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac35a37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing different lookback window sizes:\n",
      "Lookback 6: X shape (52410, 6, 13), y shape (52410, 1, 13)\n",
      "  ✅ No NaN values in window 6\n",
      "Lookback 12: X shape (52404, 12, 13), y shape (52404, 1, 13)\n",
      "  ✅ No NaN values in window 12\n",
      "Lookback 24: X shape (52392, 24, 13), y shape (52392, 1, 13)\n",
      "  ✅ No NaN values in window 24\n",
      "Lookback 48: X shape (52368, 48, 13), y shape (52368, 1, 13)\n",
      "  ✅ No NaN values in window 48\n",
      "Lookback 72: X shape (52344, 72, 13), y shape (52344, 1, 13)\n",
      "  ✅ No NaN values in window 72\n",
      "Lookback 96: X shape (52320, 96, 13), y shape (52320, 1, 13)\n",
      "  ✅ No NaN values in window 96\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(data, lookback_window, target_cols=None, forecast_horizon=1):\n",
    "    \"\"\"\n",
    "    Create sequences for time series forecasting\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame with time series data\n",
    "        lookback_window: Number of time steps to look back\n",
    "        target_cols: Columns to predict (default: all numeric columns)\n",
    "        forecast_horizon: Number of steps ahead to predict\n",
    "    \n",
    "    Returns:\n",
    "        X: Input sequences (samples, lookback_window, features)\n",
    "        y: Target values (samples, forecast_horizon, targets)\n",
    "    \"\"\"\n",
    "    if target_cols is None:\n",
    "        # Exclude datetime and index columns\n",
    "        target_cols = [col for col in data.columns if col not in ['datetime', 'Unnamed: 0']]\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(lookback_window, len(data) - forecast_horizon + 1):\n",
    "        # Input sequence\n",
    "        X.append(data[target_cols].iloc[i-lookback_window:i].values)\n",
    "        \n",
    "        # Target sequence\n",
    "        y.append(data[target_cols].iloc[i:i+forecast_horizon].values)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Test different lookback window sizes\n",
    "lookback_windows = [6, 12, 24, 48, 72, 96]  # 1 hour, 2 hours, 4 hours, 8 hours, 12 hours, 16 hours\n",
    "print(\"Testing different lookback window sizes:\")\n",
    "\n",
    "for window in lookback_windows:\n",
    "    X, y = create_sequences(df, window)\n",
    "    print(f\"Lookback {window}: X shape {X.shape}, y shape {y.shape}\")\n",
    "    \n",
    "    # Check for any NaN values\n",
    "    if np.isnan(X).any() or np.isnan(y).any():\n",
    "        print(f\"  ⚠️  Warning: NaN values found in window {window}\")\n",
    "    else:\n",
    "        print(f\"  ✅ No NaN values in window {window}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389c5b8c",
   "metadata": {},
   "source": [
    "### Raw thoughts on an optimal lookback window:\n",
    "\n",
    "- If I decide to build a univariate model (training on power consumption only), a smaller lookback window would be more appropriate, since ACF shows **autocorrelation decay** (early lags have strongest direct effects). \n",
    "    - Perhaps a window of size 3-6 values? (refer to PACF plots), the first three lags are most significant, and it would be useful to add a small safety margin beyond PACF results, still efficient\n",
    "\n",
    "- If I decide to build a multivariate model (incorporating environmental features), a larger lookback window would be more appropriate to capture the significant lag relationships in the EDA (try 24 time steps and pass in the appropriate lagged features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ba7ff",
   "metadata": {},
   "source": [
    "## Normalization + Cyclical Time Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d15edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added cyclical features:\n",
      "['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', 'day_of_year_sin', 'day_of_year_cos']\n",
      "\n",
      "Features to scale: 17\n",
      "Feature columns: ['temp', 'humidity', 'WS', 'GDF', 'DF', 'PZ1', 'PZ2', 'PZ3', 'year', 'hour_sin']...\n",
      "\n",
      "Training data size: 36691\n",
      "Total data size: 52416\n",
      "\n",
      "Scaling completed. Sample of scaled features:\n",
      "       temp  humidity        WS       GDF        DF\n",
      "0 -2.051356  0.382024 -0.812344 -0.726609 -0.636040\n",
      "1 -2.074813  0.425550 -0.812344 -0.726542 -0.636291\n",
      "2 -2.091152  0.425550 -0.813616 -0.726570 -0.636180\n",
      "3 -2.122213  0.456640 -0.812344 -0.726469 -0.636210\n",
      "4 -2.154568  0.500166 -0.813192 -0.726619 -0.636291\n"
     ]
    }
   ],
   "source": [
    "def create_cyclical_features(data):\n",
    "    \"\"\"Create cyclical time features using sine/cosine transformations\"\"\"\n",
    "    data_cyclical = data.copy()\n",
    "    \n",
    "    # Hour of day (24-hour cycle)\n",
    "    data_cyclical['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)\n",
    "    data_cyclical['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)\n",
    "    \n",
    "    # Day of week (7-day cycle)\n",
    "    data_cyclical['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
    "    data_cyclical['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
    "    \n",
    "    # Month of year (12-month cycle)\n",
    "    data_cyclical['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
    "    data_cyclical['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
    "    \n",
    "    # Day of year (365-day cycle)\n",
    "    data_cyclical['day_of_year_sin'] = np.sin(2 * np.pi * data['day_of_year'] / 365)\n",
    "    data_cyclical['day_of_year_cos'] = np.cos(2 * np.pi * data['day_of_year'] / 365)\n",
    "    \n",
    "    return data_cyclical\n",
    "\n",
    "# Create cyclical features\n",
    "df_with_cyclical = create_cyclical_features(df)\n",
    "print(\"Added cyclical features:\")\n",
    "cyclical_cols = [col for col in df_with_cyclical.columns if 'sin' in col or 'cos' in col]\n",
    "print(cyclical_cols)\n",
    "\n",
    "# Separate features for scaling\n",
    "feature_cols = [col for col in df_with_cyclical.columns \n",
    "                if col not in ['datetime', 'Unnamed: 0', 'hour', 'day_of_week', 'month', 'day_of_year']]\n",
    "\n",
    "print(f\"\\nFeatures to scale: {len(feature_cols)}\")\n",
    "print(f\"Feature columns: {feature_cols[:10]}...\")  # Show first 10\n",
    "\n",
    "# Initialize scalers\n",
    "scaler_X = StandardScaler()  # For input features\n",
    "scaler_y = StandardScaler()  # For target variables\n",
    "\n",
    "# Fit scalers on training data only (to prevent data leakage)\n",
    "train_size = int(0.7 * len(df_with_cyclical))\n",
    "train_data = df_with_cyclical.iloc[:train_size]\n",
    "\n",
    "print(f\"\\nTraining data size: {train_size}\")\n",
    "print(f\"Total data size: {len(df_with_cyclical)}\")\n",
    "\n",
    "# Fit scalers on training data\n",
    "scaler_X.fit(train_data[feature_cols])\n",
    "scaler_y.fit(train_data[feature_cols])\n",
    "\n",
    "# Scale all data\n",
    "df_scaled = df_with_cyclical.copy()\n",
    "df_scaled[feature_cols] = scaler_X.transform(df_with_cyclical[feature_cols])\n",
    "\n",
    "print(\"\\nScaling completed. Sample of scaled features:\")\n",
    "print(df_scaled[feature_cols[:5]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d525f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>WS</th>\n",
       "      <th>GDF</th>\n",
       "      <th>DF</th>\n",
       "      <th>PZ1</th>\n",
       "      <th>PZ2</th>\n",
       "      <th>PZ3</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>day_of_year_sin</th>\n",
       "      <th>day_of_year_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>-2.051356</td>\n",
       "      <td>0.382024</td>\n",
       "      <td>-0.812344</td>\n",
       "      <td>-0.726609</td>\n",
       "      <td>-0.636040</td>\n",
       "      <td>0.151445</td>\n",
       "      <td>-0.851669</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>1.414983</td>\n",
       "      <td>-1.106106</td>\n",
       "      <td>0.869011</td>\n",
       "      <td>0.397138</td>\n",
       "      <td>1.833774</td>\n",
       "      <td>-0.458845</td>\n",
       "      <td>1.74938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-01 00:10:00</td>\n",
       "      <td>-2.074813</td>\n",
       "      <td>0.425550</td>\n",
       "      <td>-0.812344</td>\n",
       "      <td>-0.726542</td>\n",
       "      <td>-0.636291</td>\n",
       "      <td>-0.433079</td>\n",
       "      <td>-0.211097</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>1.414983</td>\n",
       "      <td>-1.106106</td>\n",
       "      <td>0.869011</td>\n",
       "      <td>0.397138</td>\n",
       "      <td>1.833774</td>\n",
       "      <td>-0.458845</td>\n",
       "      <td>1.74938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-01 00:20:00</td>\n",
       "      <td>-2.091152</td>\n",
       "      <td>0.425550</td>\n",
       "      <td>-0.813616</td>\n",
       "      <td>-0.726570</td>\n",
       "      <td>-0.636180</td>\n",
       "      <td>-0.527709</td>\n",
       "      <td>-0.283791</td>\n",
       "      <td>-0.055068</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>1.414983</td>\n",
       "      <td>-1.106106</td>\n",
       "      <td>0.869011</td>\n",
       "      <td>0.397138</td>\n",
       "      <td>1.833774</td>\n",
       "      <td>-0.458845</td>\n",
       "      <td>1.74938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-01 00:30:00</td>\n",
       "      <td>-2.122213</td>\n",
       "      <td>0.456640</td>\n",
       "      <td>-0.812344</td>\n",
       "      <td>-0.726469</td>\n",
       "      <td>-0.636210</td>\n",
       "      <td>-0.651648</td>\n",
       "      <td>-0.411186</td>\n",
       "      <td>-0.174054</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>1.414983</td>\n",
       "      <td>-1.106106</td>\n",
       "      <td>0.869011</td>\n",
       "      <td>0.397138</td>\n",
       "      <td>1.833774</td>\n",
       "      <td>-0.458845</td>\n",
       "      <td>1.74938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-01 00:40:00</td>\n",
       "      <td>-2.154568</td>\n",
       "      <td>0.500166</td>\n",
       "      <td>-0.813192</td>\n",
       "      <td>-0.726619</td>\n",
       "      <td>-0.636291</td>\n",
       "      <td>-0.774750</td>\n",
       "      <td>-0.507632</td>\n",
       "      <td>-0.244729</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>1.414983</td>\n",
       "      <td>-1.106106</td>\n",
       "      <td>0.869011</td>\n",
       "      <td>0.397138</td>\n",
       "      <td>1.833774</td>\n",
       "      <td>-0.458845</td>\n",
       "      <td>1.74938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime      temp  humidity        WS       GDF        DF  \\\n",
       "0 2017-01-01 00:00:00 -2.051356  0.382024 -0.812344 -0.726609 -0.636040   \n",
       "1 2017-01-01 00:10:00 -2.074813  0.425550 -0.812344 -0.726542 -0.636291   \n",
       "2 2017-01-01 00:20:00 -2.091152  0.425550 -0.813616 -0.726570 -0.636180   \n",
       "3 2017-01-01 00:30:00 -2.122213  0.456640 -0.812344 -0.726469 -0.636210   \n",
       "4 2017-01-01 00:40:00 -2.154568  0.500166 -0.813192 -0.726619 -0.636291   \n",
       "\n",
       "        PZ1       PZ2       PZ3  day_of_week  ...  year  day_of_year  \\\n",
       "0  0.151445 -0.851669  0.033500            6  ...   0.0            1   \n",
       "1 -0.433079 -0.211097  0.016502            6  ...   0.0            1   \n",
       "2 -0.527709 -0.283791 -0.055068            6  ...   0.0            1   \n",
       "3 -0.651648 -0.411186 -0.174054            6  ...   0.0            1   \n",
       "4 -0.774750 -0.507632 -0.244729            6  ...   0.0            1   \n",
       "\n",
       "   hour_sin  hour_cos   day_sin   day_cos  month_sin  month_cos  \\\n",
       "0 -0.000725  1.414983 -1.106106  0.869011   0.397138   1.833774   \n",
       "1 -0.000725  1.414983 -1.106106  0.869011   0.397138   1.833774   \n",
       "2 -0.000725  1.414983 -1.106106  0.869011   0.397138   1.833774   \n",
       "3 -0.000725  1.414983 -1.106106  0.869011   0.397138   1.833774   \n",
       "4 -0.000725  1.414983 -1.106106  0.869011   0.397138   1.833774   \n",
       "\n",
       "   day_of_year_sin  day_of_year_cos  \n",
       "0        -0.458845          1.74938  \n",
       "1        -0.458845          1.74938  \n",
       "2        -0.458845          1.74938  \n",
       "3        -0.458845          1.74938  \n",
       "4        -0.458845          1.74938  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66ae66",
   "metadata": {},
   "source": [
    "## Convert to Tensors / train/val/test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6939cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train:36691, val:7862, test:7863\n",
      "\n",
      "Univariate shapes:\n",
      "  X_train: (36687, 3, 1)  y_train: (36687, 1)\n",
      "  X_val:   (7858, 3, 1)  y_val:   (7858, 1)\n",
      "  X_test:  (7859, 3, 1)  y_test:  (7859, 1)\n",
      "\n",
      "Multivariate shapes:\n",
      "  X_train: (36642, 48, 17)  y_train: (36642, 1)\n",
      "  X_val:   (7813, 48, 17)  y_val:   (7813, 1)\n",
      "  X_test:  (7814, 48, 17)  y_test:  (7814, 1)\n",
      "\n",
      "Batch checks:\n",
      "  Univariate batch: torch.Size([32, 3, 1]) torch.Size([32, 1])\n",
      "  Multivariate batch: torch.Size([32, 48, 17]) torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "# --- 1) Choose your targets explicitly (adjust as needed) ---\n",
    "# Example: single target (univariate) or multiple targets (multi-target)\n",
    "target_cols_uni = [\"PZ1\"]                 # univariate target\n",
    "target_cols_multi = [\"PZ1\", \"PZ2\", \"PZ3\"] # multi-target example\n",
    "\n",
    "# Sanity: ensure targets exist in the dataframe\n",
    "for c in target_cols_uni + target_cols_multi:\n",
    "    assert c in df_scaled.columns, f\"Target column {c} not found in df_scaled\"\n",
    "\n",
    "# --- 2) Define a clean, general windowing function (features vs targets) ---\n",
    "def make_windows_from_cols(\n",
    "    data: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    target_cols: list,\n",
    "    in_len: int,\n",
    "    out_horizon: int = 1,\n",
    "    out_len: int = 1,\n",
    "    seq2seq: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Build sliding windows from selected features/targets.\n",
    "    Returns X:[N,in_len,F], y:[N,K] (seq2one) or [N,out_len,K] (seq2seq).\n",
    "    \"\"\"\n",
    "    X_all = data[feature_cols].values.astype(np.float32)  # [T, F]\n",
    "    y_all = data[target_cols].values.astype(np.float32)   # [T, K]\n",
    "    T = len(data)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    last_start = T - in_len - out_horizon - out_len + 1\n",
    "    for t in range(last_start):\n",
    "        x = X_all[t : t + in_len, :]\n",
    "        start_y = t + in_len + out_horizon - 1\n",
    "        if seq2seq:\n",
    "            y = y_all[start_y : start_y + out_len, :]     # [out_len, K]\n",
    "        else:\n",
    "            y = y_all[start_y + out_len - 1, :]           # [K]\n",
    "        X_list.append(x)\n",
    "        y_list.append(y)\n",
    "    X = np.stack(X_list)\n",
    "    y = np.stack(y_list)\n",
    "    return X, y\n",
    "\n",
    "# --- 3) Minimal Dataset wrapper (uses your existing imports) ---\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "def make_dataloaders(X_train, y_train, X_val, y_val, X_test, y_test, batch_size=32):\n",
    "    ds_tr = TimeSeriesDataset(X_train, y_train)\n",
    "    ds_va = TimeSeriesDataset(X_val, y_val)\n",
    "    ds_te = TimeSeriesDataset(X_test, y_test)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=batch_size, shuffle=True,  drop_last=False)\n",
    "    dl_va = DataLoader(ds_va, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    dl_te = DataLoader(ds_te, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "    return dl_tr, dl_va, dl_te\n",
    "\n",
    "# --- 4) Chronological splits (use your existing 70% train logic) ---\n",
    "train_ratio = 0.70\n",
    "val_ratio   = 0.15\n",
    "n_total     = len(df_scaled)\n",
    "n_train     = int(n_total * train_ratio)\n",
    "n_val       = int(n_total * val_ratio)\n",
    "\n",
    "df_train = df_scaled.iloc[:n_train]\n",
    "df_val   = df_scaled.iloc[n_train:n_train+n_val]\n",
    "df_test  = df_scaled.iloc[n_train+n_val:]\n",
    "\n",
    "print(f\"Splits -> train:{len(df_train)}, val:{len(df_val)}, test:{len(df_test)}\")\n",
    "\n",
    "# --- 5) Build BOTH pipelines (univariate + multivariate) ---\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# A) Univariate (short lookback from PACF), seq2one\n",
    "in_len_uni     = 3      # your PACF idea (3–6 is fine)\n",
    "out_horizon    = 1      # next step\n",
    "out_len        = 1      # seq2one\n",
    "seq2seq_flag   = False\n",
    "\n",
    "Xtr_u, ytr_u = make_windows_from_cols(df_train, feature_cols=target_cols_uni, target_cols=target_cols_uni,\n",
    "                                      in_len=in_len_uni, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq_flag)\n",
    "Xva_u, yva_u = make_windows_from_cols(df_val,   feature_cols=target_cols_uni, target_cols=target_cols_uni,\n",
    "                                      in_len=in_len_uni, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq_flag)\n",
    "Xte_u, yte_u = make_windows_from_cols(df_test,  feature_cols=target_cols_uni, target_cols=target_cols_uni,\n",
    "                                      in_len=in_len_uni, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq_flag)\n",
    "\n",
    "uni_train_loader, uni_val_loader, uni_test_loader = make_dataloaders(\n",
    "    Xtr_u, ytr_u, Xva_u, yva_u, Xte_u, yte_u, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nUnivariate shapes:\")\n",
    "print(\"  X_train:\", Xtr_u.shape, \" y_train:\", ytr_u.shape)\n",
    "print(\"  X_val:  \", Xva_u.shape, \" y_val:  \", yva_u.shape)\n",
    "print(\"  X_test: \", Xte_u.shape, \" y_test: \", yte_u.shape)\n",
    "\n",
    "# B) Multivariate (longer lookback), seq2one\n",
    "# Use ALL scaled features as inputs, but keep targets as desired (e.g., predict PZ1)\n",
    "in_len_mv = 48  # you suggested 24; 48 is fine for richer context; tune as needed\n",
    "\n",
    "Xtr_m, ytr_m = make_windows_from_cols(df_train, feature_cols=feature_cols, target_cols=target_cols_uni,\n",
    "                                      in_len=in_len_mv, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq_flag)\n",
    "Xva_m, yva_m = make_windows_from_cols(df_val,   feature_cols=feature_cols, target_cols=target_cols_uni,\n",
    "                                      in_len=in_len_mv, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq_flag)\n",
    "Xte_m, yte_m = make_windows_from_cols(df_test,  feature_cols=feature_cols, target_cols=target_cols_uni,\n",
    "                                      in_len=in_len_mv, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq_flag)\n",
    "\n",
    "multi_train_loader, multi_val_loader, multi_test_loader = make_dataloaders(\n",
    "    Xtr_m, ytr_m, Xva_m, yva_m, Xte_m, yte_m, batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(\"\\nMultivariate shapes:\")\n",
    "print(\"  X_train:\", Xtr_m.shape, \" y_train:\", ytr_m.shape)\n",
    "print(\"  X_val:  \", Xva_m.shape, \" y_val:  \", yva_m.shape)\n",
    "print(\"  X_test: \", Xte_m.shape, \" y_test: \", yte_m.shape)\n",
    "\n",
    "# --- 6) Quick sanity check batch ---\n",
    "xb_u, yb_u = next(iter(uni_train_loader))\n",
    "xb_m, yb_m = next(iter(multi_train_loader))\n",
    "print(\"\\nBatch checks:\")\n",
    "print(\"  Univariate batch:\", xb_u.shape, yb_u.shape)  # [B, in_len_uni, 1], [B, 1]\n",
    "print(\"  Multivariate batch:\", xb_m.shape, yb_m.shape) # [B, in_len_mv, F], [B, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414939c",
   "metadata": {},
   "source": [
    "When we build lookback windows, the result is always 3D:\n",
    "\n",
    "X∈[samples,time steps,features]\n",
    "\n",
    "- Axis 0 (rows) = samples\n",
    "    - Each row is one training example. If you have 10,000 possible sliding windows in your train set, then X.shape[0] = 10,000.\n",
    "\n",
    "- Axis 1 (columns in time) = lookback window length\n",
    "    - Each sample contains a sequence of timesteps.\n",
    "    - Example: if lookback_window = 48, each row covers 48 past time steps.\n",
    "\n",
    "- Axis 2 (features) = number of input features\n",
    "    - Univariate (only power consumption): features = 1.\n",
    "    - Multivariate (power + weather + cyclical time): features = many."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc956920",
   "metadata": {},
   "source": [
    "## PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bf7691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splits -> train:36691, val:7862, test:7863\n",
      "Shapes:\n",
      "  X_train: (36666, 24, 17)  y_train: (36666, 1)\n",
      "  X_val:   (7837, 24, 17)  y_val:   (7837, 1)\n",
      "  X_test:  (7838, 24, 17)  y_test:  (7838, 1)\n",
      "\n",
      "Batch check:\n",
      "  X batch: torch.Size([32, 24, 17])\n",
      "  y batch: torch.Size([32, 1])\n",
      "\n",
      "Model IO -> input_size=17, output_size=1, in_len=24, out_len=1, seq2seq=False\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PyTorch DataLoaders (drop-in)\n",
    "# =========================\n",
    "\n",
    "# --- Choose what to predict ---\n",
    "# For single-target:\n",
    "target_cols = [\"PZ1\"]\n",
    "# For multi-target (predict all three at once), uncomment:\n",
    "# target_cols = [\"PZ1\", \"PZ2\", \"PZ3\"]\n",
    "\n",
    "# --- Windowing config ---\n",
    "in_len       = 24      # lookback steps (e.g., 24*10min ≈ 4 hours)\n",
    "out_horizon  = 1       # 1 = predict next step; >1 = skip ahead\n",
    "out_len      = 1       # 1 = seq2one; >1 = seq2seq (multi-step)\n",
    "seq2seq      = False   # True if you want multi-step output\n",
    "\n",
    "# --- Chronological split (70/15/15) ---\n",
    "n_total  = len(df_scaled)\n",
    "n_train  = int(0.70 * n_total)\n",
    "n_val    = int(0.15 * n_total)\n",
    "\n",
    "df_train = df_scaled.iloc[:n_train]\n",
    "df_val   = df_scaled.iloc[n_train:n_train+n_val]\n",
    "df_test  = df_scaled.iloc[n_train+n_val:]\n",
    "\n",
    "print(f\"Splits -> train:{len(df_train)}, val:{len(df_val)}, test:{len(df_test)}\")\n",
    "\n",
    "# --- Sliding windows AFTER split (no leakage) ---\n",
    "def make_windows_from_cols(\n",
    "    data: pd.DataFrame,\n",
    "    feature_cols: list,\n",
    "    target_cols: list,\n",
    "    in_len: int,\n",
    "    out_horizon: int = 1,\n",
    "    out_len: int = 1,\n",
    "    seq2seq: bool = False\n",
    "):\n",
    "    X_all = data[feature_cols].values.astype(np.float32)  # [T, F]\n",
    "    y_all = data[target_cols].values.astype(np.float32)   # [T, K]\n",
    "    T = len(data)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    last_start = T - in_len - out_horizon - out_len + 1\n",
    "    for t in range(max(0, last_start)):\n",
    "        x = X_all[t : t + in_len, :]\n",
    "        start_y = t + in_len + out_horizon - 1\n",
    "        if seq2seq:\n",
    "            y = y_all[start_y : start_y + out_len, :]     # [out_len, K]\n",
    "        else:\n",
    "            y = y_all[start_y + out_len - 1, :]           # [K]\n",
    "        X_list.append(x)\n",
    "        y_list.append(y)\n",
    "    X = np.stack(X_list) if X_list else np.empty((0, in_len, len(feature_cols)), dtype=np.float32)\n",
    "    if seq2seq:\n",
    "        y = np.stack(y_list) if y_list else np.empty((0, out_len, len(target_cols)), dtype=np.float32)\n",
    "    else:\n",
    "        y = np.stack(y_list) if y_list else np.empty((0, len(target_cols)), dtype=np.float32)\n",
    "    return X, y\n",
    "\n",
    "# Inputs for the model:\n",
    "# - Multivariate model: use ALL scaled features as inputs\n",
    "# - If you want a strictly univariate INPUT as well, set feature_cols = target_cols\n",
    "Xtr, ytr = make_windows_from_cols(df_train, feature_cols=feature_cols, target_cols=target_cols,\n",
    "                                  in_len=in_len, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq)\n",
    "Xva, yva = make_windows_from_cols(df_val,   feature_cols=feature_cols, target_cols=target_cols,\n",
    "                                  in_len=in_len, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq)\n",
    "Xte, yte = make_windows_from_cols(df_test,  feature_cols=feature_cols, target_cols=target_cols,\n",
    "                                  in_len=in_len, out_horizon=out_horizon, out_len=out_len, seq2seq=seq2seq)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", Xtr.shape, \" y_train:\", ytr.shape)\n",
    "print(\"  X_val:  \", Xva.shape, \" y_val:  \", yva.shape)\n",
    "print(\"  X_test: \", Xte.shape, \" y_test: \", yte.shape)\n",
    "\n",
    "# --- Minimal Dataset + DataLoaders ---\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "def make_dataloaders(Xtr, ytr, Xva, yva, Xte, yte, batch_size=32, shuffle_train=True):\n",
    "    ds_tr = TimeSeriesDataset(Xtr, ytr)\n",
    "    ds_va = TimeSeriesDataset(Xva, yva)\n",
    "    ds_te = TimeSeriesDataset(Xte, yte)\n",
    "    # CUDA-friendly defaults\n",
    "    dl_kwargs = dict(batch_size=batch_size, drop_last=False, pin_memory=torch.cuda.is_available())\n",
    "    dl_tr = DataLoader(ds_tr, shuffle=shuffle_train, **dl_kwargs)\n",
    "    dl_va = DataLoader(ds_va, shuffle=False,          **dl_kwargs)\n",
    "    dl_te = DataLoader(ds_te, shuffle=False,          **dl_kwargs)\n",
    "    return dl_tr, dl_va, dl_te\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader, val_loader, test_loader = make_dataloaders(Xtr, ytr, Xva, yva, Xte, yte, batch_size=BATCH_SIZE)\n",
    "\n",
    "# --- Quick sanity check batch ---\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"\\nBatch check:\")\n",
    "print(\"  X batch:\", xb.shape)  # [B, in_len, F]\n",
    "print(\"  y batch:\", yb.shape)  # [B, K] or [B, out_len, K] if seq2seq\n",
    "\n",
    "# --- Metadata for your model definition ---\n",
    "in_features  = xb.shape[-1]                 # F\n",
    "out_targets  = yb.shape[-1] if not seq2seq else yb.shape[-1]  # K\n",
    "print(f\"\\nModel IO -> input_size={in_features}, output_size={out_targets}, in_len={in_len}, out_len={out_len}, seq2seq={seq2seq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6038229b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
