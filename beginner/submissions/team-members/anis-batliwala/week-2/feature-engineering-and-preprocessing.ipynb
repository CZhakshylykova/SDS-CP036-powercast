{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a443f22b",
   "metadata": {},
   "source": [
    "# Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b8dc16",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6206cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure the common module can be imported\n",
    "# Adjust the path to point to the common directory\n",
    "# Get the parent directory of the project (one level above week-2/)\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "from common.load_data import load_data\n",
    "from common.time_based_features import add_time_based_features\n",
    "from common.lag_and_rolling_statistics import engineer_lag_and_rolling\n",
    "from common.scale_features import scale_features\n",
    "from common.chronological_split import chronological_train_test_split\n",
    "from common.linear_regression import baseline_model_performance\n",
    "from common.persistence_model import persistence_baseline_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7284d161",
   "metadata": {},
   "source": [
    "### Pipeline Overview:\n",
    "- Load dataset from CSV and parse datetime index (ensuring chronological order)\n",
    "- Add time-based features (month, weekday, hour, is_weekend) to help capture temporal patterns\n",
    "- Engineer lagged and rolling statistical features per zone to provide historical context\n",
    "- split the dataset chronologically into train and test sets to prevent data leakage and preserve time sequence\n",
    "- finally, normalize numerical features (including engineered features) using StandardScaler for stable training\n",
    "Note: All preprocessing is done BEFORE the train/test split to ensure consistent feature transformations.\n",
    "The chronological split maintains the temporal integrity essential for time series modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92a09dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using 'DateTime' as datetime index column\n",
      "\n",
      "Performance metrics for Zone 1 Power Consumption:\n",
      "  Metric       Value\n",
      "0   RMSE  472.336672\n",
      "1    MAE  341.562069\n",
      "2     R2    0.994133\n",
      "\n",
      "Performance metrics for Zone 2 Power Consumption:\n",
      "  Metric       Value\n",
      "0   RMSE  393.182648\n",
      "1    MAE  290.227347\n",
      "2     R2    0.994859\n",
      "\n",
      "Performance metrics for Zone 3 Power Consumption:\n",
      "  Metric       Value\n",
      "0   RMSE  294.330922\n",
      "1    MAE  185.703733\n",
      "2     R2    0.992042\n"
     ]
    }
   ],
   "source": [
    "zones = [\n",
    "    'Zone 1 Power Consumption',\n",
    "    'Zone 2 Power Consumption',\n",
    "    'Zone 3 Power Consumption'\n",
    "]\n",
    "\n",
    "# Load and preprocess data\n",
    "dataset = (\n",
    "    load_data(file_path='..\\Data\\Tetuan City power consumption.csv')\n",
    "    .pipe(add_time_based_features)\n",
    ")\n",
    "\n",
    "# Split BEFORE creating lags/rolling\n",
    "dataset_train, dataset_test = chronological_train_test_split(dataset, split_ratio=0.8)\n",
    "\n",
    "for zone in zones:\n",
    "    # Feature engineering ONLY for this zone\n",
    "    train_zone = engineer_lag_and_rolling(dataset_train, zone, lags=[1], rolling_windows=[3])\n",
    "    test_zone = engineer_lag_and_rolling(dataset_test, zone, lags=[1], rolling_windows=[3])\n",
    "\n",
    "    # Scale features\n",
    "    train_scaled, test_scaled, scaler, feature_cols = scale_features(\n",
    "        train_zone, test_zone, target_col=zone\n",
    "    )\n",
    "\n",
    "    # Train & evaluate\n",
    "    performance = baseline_model_performance(\n",
    "        train_dataset=train_scaled,\n",
    "        test_dataset=test_scaled,\n",
    "        target_col=zone,\n",
    "        feature_cols=feature_cols\n",
    "    )\n",
    "\n",
    "    print(f\"\\nPerformance metrics for {zone}:\")\n",
    "    print(performance['metrics'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e361423d",
   "metadata": {},
   "source": [
    "**Feature Columns** \n",
    "- Zone 1\n",
    "  - hour\n",
    "  - weekday\n",
    "  - is_weekend\n",
    "  - month\n",
    "  - Zone 1 Power Consumption_lag1\n",
    "  - Zone 1 Power Consumption_roll_mean3\n",
    "  - Zone 1 Power Consumption_roll_std3\n",
    "  - Zone 1 Power Consumption_roll_median3\n",
    "\n",
    "- Zone 2\n",
    "  - hour\n",
    "  - weekday\n",
    "  - is_weekend\n",
    "  - month\n",
    "  - Zone 2 Power Consumption_lag1\n",
    "  - Zone 2 Power Consumption_roll_mean3\n",
    "  - Zone 2 Power Consumption_roll_std3\n",
    "  - Zone 2 Power Consumption_roll_median3\n",
    "\n",
    "- Zone 3\n",
    "  - hour\n",
    "  - weekday\n",
    "  - is_weekend\n",
    "  - month\n",
    "  - Zone 3 Power Consumption_lag1\n",
    "  - Zone 3 Power Consumption_roll_mean3\n",
    "  - Zone 3 Power Consumption_roll_std3\n",
    "  - Zone 3 Power Consumption_roll_median3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056a848e",
   "metadata": {},
   "source": [
    "#### Interpretation per zone\n",
    "**RMSE**: Root Mean Squared Error  \n",
    "**MAE**: Mean Absolute Error  \n",
    "**R2**: Coefficient of Determination - a statistical measure that tells how much of the variance in the target variable.\n",
    "\n",
    "\n",
    "- **Zone 1**\n",
    "  - RMSE: ~472.34\n",
    "  - MAE: ~341.56\n",
    "  - R²: 0.9941 → The model explains 99.42% of the variance, which is huge (near-perfect).\n",
    "\n",
    "- **Zone 2**\n",
    "  - RMSE: ~393.18\n",
    "  - MAE: ~290.23\n",
    "  - R²: 0.9948 → Best-performing zone, suggesting Zone 2’s patterns are the most predictable from its own history.\n",
    "\n",
    "- **Zone 3**\n",
    "  - RMSE: ~294.33\n",
    "  - MAE: ~185.70\n",
    "  - R²: 0.9920 → Slightly more error than Zone 2 but still very high predictive power.\n",
    "\n",
    "In time series, high R² is due to strong autocorrelation (yesterday’s consumption a very good predictor for today).\n",
    "\n",
    "Adding Persistence model = baseline of “no model” — just using previous value as prediction - to compare how the linear regression baseline model is compared to baseline persistence model - using yesterday's value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ac5f80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence baseline metrics for Zone 1 Power Consumption:\n",
      "  Metric       Value\n",
      "0   RMSE  549.974681\n",
      "1    MAE  369.605858\n",
      "2     R2    0.992046\n",
      "\n",
      "Persistence baseline metrics for Zone 2 Power Consumption:\n",
      "  Metric       Value\n",
      "0   RMSE  459.747095\n",
      "1    MAE  307.673258\n",
      "2     R2    0.992971\n",
      "\n",
      "Persistence baseline metrics for Zone 3 Power Consumption:\n",
      "  Metric       Value\n",
      "0   RMSE  338.884857\n",
      "1    MAE  201.406095\n",
      "2     R2    0.989449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for zone in zones:\n",
    "    # Feature engineering ONLY for this zone\n",
    "    test_zone = engineer_lag_and_rolling(dataset_test, zone, lags=[1, 3], rolling_windows=[3, 7])\n",
    "\n",
    "    metrics = persistence_baseline_performance(\n",
    "        test_zone, target_col=zone, lag=1)\n",
    "    print(f\"Persistence baseline metrics for {zone}:\")\n",
    "    print(metrics)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88c2384",
   "metadata": {},
   "source": [
    "### Analysis of Linear Regression vs. Persistence Model Performance\n",
    "\n",
    "From the above results - `Linear Regression (lag-1)` model significantly out performs the `Persistence (naïve)` model across all zones.\n",
    "\n",
    "\n",
    "| Zone   | Model              | RMSE     | MAE      | R²       | Improvement vs. Persistence               |\n",
    "|--------|--------------------|----------|----------|----------|------------------------------------------|\n",
    "| Zone 1 | Linear Regression  | 472.34   | 341.56   | 0.9941   | RMSE ↓14.1%, MAE ↓7.6%, R² ↑0.21%       |\n",
    "| Zone 1 | Persistence        | 549.97   | 369.61   | 0.9920   | (Baseline)                               |\n",
    "| Zone 2 | Linear Regression  | 393.18   | 290.23   | 0.9949   | RMSE ↓14.5%, MAE ↓5.7%, R² ↑0.19%       |\n",
    "| Zone 2 | Persistence        | 459.75   | 307.67   | 0.9930   | (Baseline)                               |\n",
    "| Zone 3 | Linear Regression  | 294.33   | 185.70   | 0.9920   | RMSE ↓13.2%, MAE ↓7.8%, R² ↑0.26%       |\n",
    "| Zone 3 | Persistence        | 338.88   | 201.41   | 0.9894   | (Baseline)                               |                       |\n",
    "\n",
    "**Persistence model** (minimal benchmark) is used to verify that LinearRegression baseline model adds value (which it does).  \n",
    "**Data** has structure (trends/scaling effects) that Persistence misses.  \n",
    "**Data Characteristics**\n",
    "- Strong autocorrelation: The next value heavily depends on the current one.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SDS-CP036-powercast (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
