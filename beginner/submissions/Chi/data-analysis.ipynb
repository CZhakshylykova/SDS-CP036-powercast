{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22192bbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"import pandas as pd\\n\",\n",
    "        \"import numpy as np\\n\",\n",
    "        \"import os\\n\",\n",
    "        \"import matplotlib.pyplot as plt\\n\",\n",
    "        \"import seaborn as sns\\n\",\n",
    "        \"import json\\n\",\n",
    "        \"import logging\\n\",\n",
    "        \"import warnings\\n\",\n",
    "        \"warnings.filterwarnings('ignore')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Setup logging\\n\",\n",
    "        \"logging.basicConfig(filename='data_analysis.log', level=logging.DEBUG,\\n\",\n",
    "        \"                    format='%(asctime)s - %(levelname)s - %(message)s')\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Configuration\\n\",\n",
    "        \"CONFIG = {\\n\",\n",
    "        \"    'RAW_DATA_PATH': 'data/raw/Tetuan City power consumption.csv',\\n\",\n",
    "        \"    'MODEL_DIR': 'models',\\n\",\n",
    "        \"    'RESULTS_DIR': 'results',\\n\",\n",
    "        \"    'MODEL_PATHS': {\\n\",\n",
    "        \"        'Zone_1_Power_Consumption': 'models/best_model_Zone_1_Power_Consumption.pkl',\\n\",\n",
    "        \"        'Zone_2_Power_Consumption': 'models/best_model_Zone_2_Power_Consumption.pkl',\\n\",\n",
    "        \"        'Zone_3_Power_Consumption': 'models/best_model_Zone_3_Power_Consumption.pkl',\\n\",\n",
    "        \"    },\\n\",\n",
    "        \"    'FEATURES': ['Temperature', 'Humidity', 'Wind_Speed', 'general_diffuse_flows', 'diffuse_flows'],\\n\",\n",
    "        \"    'TARGETS': ['Zone_1_Power_Consumption', 'Zone_2_Power_Consumption', 'Zone_3_Power_Consumption']\\n\",\n",
    "        \"}\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Create directories\\n\",\n",
    "        \"os.makedirs(CONFIG['RESULTS_DIR'], exist_ok=True)\\n\",\n",
    "        \"os.makedirs(CONFIG['MODEL_DIR'], exist_ok=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"def json_serializable(obj):\\n\",\n",
    "        \"    \\\"\\\"\\\"Handle non-serializable objects for JSON.\\\"\\\"\\\"\\n\",\n",
    "        \"    if pd.isna(obj) or obj is np.nan:\\n\",\n",
    "        \"        return None\\n\",\n",
    "        \"    if isinstance(obj, (np.integer, np.floating)):\\n\",\n",
    "        \"        return obj.item()\\n\",\n",
    "        \"    if isinstance(obj, np.ndarray):\\n\",\n",
    "        \"        return obj.tolist()\\n\",\n",
    "        \"    if isinstance(obj, pd.Timestamp):\\n\",\n",
    "        \"        return obj.strftime('%Y-%m-%d %H:%M:%S')\\n\",\n",
    "        \"    return str(obj)\\n\",\n",
    "        \"\\n\",\n",
    "        \"def load_data(file_path):\\n\",\n",
    "        \"    \\\"\\\"\\\"Load the dataset from a CSV file and inspect its structure.\\\"\\\"\\\"\\n\",\n",
    "        \"    logging.info(f\\\"Attempting to load dataset from {file_path}\\\")\\n\",\n",
    "        \"    if not os.path.exists(file_path):\\n\",\n",
    "        \"        logging.warning(f\\\"Dataset not found at {file_path}. Using sample data.\\\")\\n\",\n",
    "        \"        dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='10min')\\n\",\n",
    "        \"        np.random.seed(42)\\n\",\n",
    "        \"        df = pd.DataFrame({\\n\",\n",
    "        \"            'Temperature': np.random.normal(20, 5, len(dates)),\\n\",\n",
    "        \"            'Humidity': np.random.uniform(30, 80, len(dates)),\\n\",\n",
    "        \"            'Wind_Speed': np.random.exponential(2, len(dates)),\\n\",\n",
    "        \"            'general_diffuse_flows': np.random.uniform(0, 500, len(dates)),\\n\",\n",
    "        \"            'diffuse_flows': np.random.uniform(0, 300, len(dates)),\\n\",\n",
    "        \"            'Zone_1_Power_Consumption': np.random.normal(100, 20, len(dates)),\\n\",\n",
    "        \"            'Zone_2_Power_Consumption': np.random.normal(80, 15, len(dates)),\\n\",\n",
    "        \"            'Zone_3_Power_Consumption': np.random.normal(60, 10, len(dates))\\n\",\n",
    "        \"        }, index=dates)\\n\",\n",
    "        \"        logging.info(f\\\"Sample data created. Columns: {df.columns.tolist()}\\\")\\n\",\n",
    "        \"        logging.info(f\\\"Missing values:\\\\n{df.isnull().sum().to_dict()}\\\")\\n\",\n",
    "        \"        logging.info(f\\\"Duplicate timestamps: {df.index.duplicated().sum()}\\\")\\n\",\n",
    "        \"        return df\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        df = pd.read_csv(file_path, parse_dates=['DateTime'], index_col='DateTime')\\n\",\n",
    "        \"        logging.info(f\\\"Loaded dataset with shape: {df.shape}\\\")\\n\",\n",
    "        \"        logging.info(f\\\"Columns: {df.columns.tolist()}\\\")\\n\",\n",
    "        \"        logging.info(f\\\"Missing values:\\\\n{df.isnull().sum().to_dict()}\\\")\\n\",\n",
    "        \"        logging.info(f\\\"Duplicate timestamps: {df.index.duplicated().sum()}\\\")\\n\",\n",
    "        \"        return df\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error loading data: {e}\\\")\\n\",\n",
    "        \"        return None\\n\",\n",
    "        \"\\n\",\n",
    "        \"def clean_data(df):\\n\",\n",
    "        \"    \\\"\\\"\\\"Clean the dataset by handling missing values, standardizing column names, and removing duplicates.\\\"\\\"\\\"\\n\",\n",
    "        \"    if df is None or df.empty:\\n\",\n",
    "        \"        logging.error(\\\"Input DataFrame is None or empty. Returning empty DataFrame.\\\")\\n\",\n",
    "        \"        return pd.DataFrame(), {'duplicate_count': 0, 'duplicates': []}\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Standardize column names\\n\",\n",
    "        \"    column_mapping = {\\n\",\n",
    "        \"        'Zone 1 Power Consumption': 'Zone_1_Power_Consumption',\\n\",\n",
    "        \"        'Zone 2  Power Consumption': 'Zone_2_Power_Consumption',\\n\",\n",
    "        \"        'Zone 3  Power Consumption': 'Zone_3_Power_Consumption',\\n\",\n",
    "        \"        'Wind Speed': 'Wind_Speed',\\n\",\n",
    "        \"        'general diffuse flows': 'general_diffuse_flows',\\n\",\n",
    "        \"        'diffuse flows': 'diffuse_flows'\\n\",\n",
    "        \"    }\\n\",\n",
    "        \"    df = df.rename(columns=column_mapping)\\n\",\n",
    "        \"    df.columns = [col.replace(' ', '_') for col in df.columns]\\n\",\n",
    "        \"    logging.info(f\\\"Columns after renaming: {df.columns.tolist()}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Verify required columns\\n\",\n",
    "        \"    required_cols = CONFIG['FEATURES'] + CONFIG['TARGETS']\\n\",\n",
    "        \"    missing_cols = [col for col in required_cols if col not in df.columns]\\n\",\n",
    "        \"    if missing_cols:\\n\",\n",
    "        \"        logging.error(f\\\"Missing required columns: {missing_cols}\\\")\\n\",\n",
    "        \"        raise ValueError(f\\\"Missing required columns: {missing_cols}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Ensure numeric data types\\n\",\n",
    "        \"    for col in CONFIG['FEATURES'] + CONFIG['TARGETS']:\\n\",\n",
    "        \"        df[col] = pd.to_numeric(df[col], errors='coerce')\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Handle missing values\\n\",\n",
    "        \"    if df.isnull().any().any():\\n\",\n",
    "        \"        logging.info(f\\\"Found {df.isnull().sum().sum()} missing values. Filling with mean.\\\")\\n\",\n",
    "        \"        df = df.fillna(df.mean(numeric_only=True))\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Handle negative values in power consumption\\n\",\n",
    "        \"    for zone in CONFIG['TARGETS']:\\n\",\n",
    "        \"        if (df[zone] < 0).any():\\n\",\n",
    "        \"            logging.info(f\\\"Negative values found in {zone}. Replacing with 0.\\\")\\n\",\n",
    "        \"            df[zone] = df[zone].clip(lower=0)\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Handle duplicate timestamps\\n\",\n",
    "        \"    if df.index.duplicated().any():\\n\",\n",
    "        \"        duplicate_count = df.index.duplicated().sum()\\n\",\n",
    "        \"        logging.info(f\\\"Found {duplicate_count} duplicate timestamps. Aggregating by mean.\\\")\\n\",\n",
    "        \"        duplicate_indices = df.index[df.index.duplicated()].strftime('%Y-%m-%d %H:%M:%S').tolist()\\n\",\n",
    "        \"        df = df.groupby(df.index).mean()\\n\",\n",
    "        \"        logging.info(f\\\"After aggregation, dataset shape: {df.shape}\\\")\\n\",\n",
    "        \"        return df, {'duplicate_count': duplicate_count, 'duplicates': duplicate_indices[:5]}\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        logging.info(\\\"No duplicate timestamps found.\\\")\\n\",\n",
    "        \"        return df, {'duplicate_count': 0, 'duplicates': []}\\n\",\n",
    "        \"\\n\",\n",
    "        \"# Load and clean data\\n\",\n",
    "        \"try:\\n\",\n",
    "        \"    df = load_data(CONFIG['RAW_DATA_PATH'])\\n\",\n",
    "        \"    df, duplicate_info = clean_data(df)\\n\",\n",
    "        \"    logging.info(f\\\"Cleaned data head:\\\\n{df.head().to_dict()}\\\")\\n\",\n",
    "        \"except Exception as e:\\n\",\n",
    "        \"    logging.error(f\\\"Failed to load or clean data: {e}\\\")\\n\",\n",
    "        \"    df = pd.DataFrame()\\n\",\n",
    "        \"    duplicate_info = {'duplicate_count': 0, 'duplicates': []}\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Time Consistency & Structure\\n\",\n",
    "        \"def check_time_consistency(df, duplicate_info):\\n\",\n",
    "        \"    results = {\\n\",\n",
    "        \"        'timestamp_consistency': {'is_monotonic': False, 'irregular_timestamps': 'N/A'},\\n\",\n",
    "        \"        'sampling_frequency': {'frequency_minutes': 'N/A', 'is_consistent': False},\\n\",\n",
    "        \"        'duplicates': duplicate_info\\n\",\n",
    "        \"    }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    if df is not None and not df.empty:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            # Check for monotonic timestamps\\n\",\n",
    "        \"            is_monotonic = df.index.is_monotonic_increasing\\n\",\n",
    "        \"            results['timestamp_consistency']['is_monotonic'] = is_monotonic\\n\",\n",
    "        \"            results['timestamp_consistency']['irregular_timestamps'] = 0 if is_monotonic else len(df.index[df.index.to_series().diff().dt.total_seconds() <= 0])\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            # Check sampling frequency\\n\",\n",
    "        \"            time_diffs = df.index.to_series().diff().dropna().dt.total_seconds() / 60\\n\",\n",
    "        \"            expected_freq = 10  # 10-minute intervals\\n\",\n",
    "        \"            is_consistent = (time_diffs == expected_freq).all() if not time_diffs.empty else False\\n\",\n",
    "        \"            results['sampling_frequency']['frequency_minutes'] = time_diffs.median() if not time_diffs.empty else 'N/A'\\n\",\n",
    "        \"            results['sampling_frequency']['is_consistent'] = is_consistent\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error in time consistency analysis: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save results\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        file_path = os.path.join(CONFIG['RESULTS_DIR'], 'time_results.json')\\n\",\n",
    "        \"        with open(file_path, 'w') as f:\\n\",\n",
    "        \"            json.dump(results, f, indent=2, default=json_serializable)\\n\",\n",
    "        \"        logging.info(f\\\"Time consistency results saved to {file_path}\\\")\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error saving time_results.json: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return results\\n\",\n",
    "        \"\\n\",\n",
    "        \"time_results = check_time_consistency(df, duplicate_info)\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Temporal Patterns\\n\",\n",
    "        \"def analyze_temporal_patterns(df):\\n\",\n",
    "        \"    results = {\\n\",\n",
    "        \"        'hourly': {target: {'mean': {}, 'std': {}} for target in CONFIG['TARGETS']},\\n\",\n",
    "        \"        'daily': {target: {'mean': {}, 'std': {}} for target in CONFIG['TARGETS']},\\n\",\n",
    "        \"        'weekly': {target: {'mean': {}, 'std': {}} for target in CONFIG['TARGETS']}\\n\",\n",
    "        \"    }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    if df is not None and not df.empty:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            for target in CONFIG['TARGETS']:\\n\",\n",
    "        \"                if target not in df.columns:\\n\",\n",
    "        \"                    logging.warning(f\\\"{target} not in DataFrame columns. Skipping.\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"                # Hourly patterns\\n\",\n",
    "        \"                hourly = df[target].groupby(df.index.hour).agg(['mean', 'std']).to_dict()\\n\",\n",
    "        \"                results['hourly'][target] = {'mean': hourly['mean'], 'std': hourly['std']}\\n\",\n",
    "        \"                # Daily patterns\\n\",\n",
    "        \"                daily = df[target].groupby(df.index.dayofweek).agg(['mean', 'std']).to_dict()\\n\",\n",
    "        \"                results['daily'][target] = {'mean': daily['mean'], 'std': daily['std']}\\n\",\n",
    "        \"                # Weekly patterns\\n\",\n",
    "        \"                weekly = df[target].resample('W').agg(['mean', 'std']).to_dict()\\n\",\n",
    "        \"                results['weekly'][target] = {'mean': weekly['mean'], 'std': weekly['std']}\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error in temporal patterns analysis: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save results\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        file_path = os.path.join(CONFIG['RESULTS_DIR'], 'temporal_results.json')\\n\",\n",
    "        \"        with open(file_path, 'w') as f:\\n\",\n",
    "        \"            json.dump(results, f, indent=2, default=json_serializable)\\n\",\n",
    "        \"        logging.info(f\\\"Temporal patterns results saved to {file_path}\\\")\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error saving temporal_results.json: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return results\\n\",\n",
    "        \"\\n\",\n",
    "        \"temporal_results = analyze_temporal_patterns(df)\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Correlation Analysis\\n\",\n",
    "        \"def correlation_analysis(df):\\n\",\n",
    "        \"    results = {target: {feature: 'N/A' for feature in CONFIG['FEATURES']} for target in CONFIG['TARGETS']}\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    if df is not None and not df.empty:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            for target in CONFIG['TARGETS']:\\n\",\n",
    "        \"                if target not in df.columns:\\n\",\n",
    "        \"                    logging.warning(f\\\"{target} not in DataFrame columns. Skipping.\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"                for feature in CONFIG['FEATURES']:\\n\",\n",
    "        \"                    if feature in df.columns:\\n\",\n",
    "        \"                        corr = df[target].corr(df[feature])\\n\",\n",
    "        \"                        results[target][feature] = corr if not pd.isna(corr) else 'N/A'\\n\",\n",
    "        \"                    else:\\n\",\n",
    "        \"                        logging.warning(f\\\"{feature} not in DataFrame columns. Skipping.\\\")\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error in correlation analysis: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save results\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        file_path = os.path.join(CONFIG['RESULTS_DIR'], 'correlation_results.json')\\n\",\n",
    "        \"        with open(file_path, 'w') as f:\\n\",\n",
    "        \"            json.dump(results, f, indent=2, default=json_serializable)\\n\",\n",
    "        \"        logging.info(f\\\"Correlation results saved to {file_path}\\\")\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error saving correlation_results.json: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return results\\n\",\n",
    "        \"\\n\",\n",
    "        \"correlation_results = correlation_analysis(df)\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Lag Effects\\n\",\n",
    "        \"def analyze_lag_effects(df, max_lag=24):\\n\",\n",
    "        \"    results = {target: {str(lag): 'N/A' for lag in range(1, max_lag + 1)} for target in CONFIG['TARGETS']}\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    if df is not None and not df.empty:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            for target in CONFIG['TARGETS']:\\n\",\n",
    "        \"                if target not in df.columns:\\n\",\n",
    "        \"                    logging.warning(f\\\"{target} not in DataFrame columns. Skipping.\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"                for lag in range(1, max_lag + 1):\\n\",\n",
    "        \"                    lagged = df[target].shift(lag)\\n\",\n",
    "        \"                    corr = df[target].corr(lagged)\\n\",\n",
    "        \"                    results[target][str(lag)] = corr if not pd.isna(corr) else 'N/A'\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error in lag effects analysis: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save results\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        file_path = os.path.join(CONFIG['RESULTS_DIR'], 'lagged_results.json')\\n\",\n",
    "        \"        with open(file_path, 'w') as f:\\n\",\n",
    "        \"            json.dump(results, f, indent=2, default=json_serializable)\\n\",\n",
    "        \"        logging.info(f\\\"Lagged results saved to {file_path}\\\")\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error saving lagged_results.json: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return results\\n\",\n",
    "        \"\\n\",\n",
    "        \"lagged_results = analyze_lag_effects(df)\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Outlier Detection\\n\",\n",
    "        \"def detect_outliers(df):\\n\",\n",
    "        \"    results = {col: {'outlier_count': 0, 'outlier_indices': [], 'summary': ''} for col in CONFIG['FEATURES'] + CONFIG['TARGETS']}\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    if df is not None and not df.empty:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            # Generate boxplot for visualization\\n\",\n",
    "        \"            available_cols = [col for col in CONFIG['FEATURES'] + CONFIG['TARGETS'] if col in df.columns]\\n\",\n",
    "        \"            if available_cols:\\n\",\n",
    "        \"                plt.figure(figsize=(10, 6))\\n\",\n",
    "        \"                df[available_cols].boxplot()\\n\",\n",
    "        \"                plt.title(\\\"Boxplot for Outlier Detection\\\", fontsize=14, fontweight='bold')\\n\",\n",
    "        \"                plt.xticks(rotation=45, ha='right')\\n\",\n",
    "        \"                plt.ylabel(\\\"Values\\\")\\n\",\n",
    "        \"                boxplot_path = os.path.join(CONFIG['RESULTS_DIR'], 'outlier_boxplot.png')\\n\",\n",
    "        \"                plt.savefig(boxplot_path)\\n\",\n",
    "        \"                plt.close()\\n\",\n",
    "        \"                results['boxplot'] = boxplot_path\\n\",\n",
    "        \"                logging.info(f\\\"Boxplot saved to {boxplot_path}\\\")\\n\",\n",
    "        \"            else:\\n\",\n",
    "        \"                logging.warning(\\\"No valid columns available for boxplot.\\\")\\n\",\n",
    "        \"            \\n\",\n",
    "        \"            for col in CONFIG['FEATURES'] + CONFIG['TARGETS']:\\n\",\n",
    "        \"                if col not in df.columns:\\n\",\n",
    "        \"                    results[col]['summary'] = f\\\"Column {col} not found in dataset.\\\"\\n\",\n",
    "        \"                    logging.warning(f\\\"{col} not in DataFrame columns. Skipping outlier detection.\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"                if not pd.api.types.is_numeric_dtype(df[col]):\\n\",\n",
    "        \"                    results[col]['summary'] = f\\\"Column {col} is non-numeric, skipping outlier detection.\\\"\\n\",\n",
    "        \"                    logging.warning(f\\\"{col} is non-numeric, skipping outlier detection.\\\")\\n\",\n",
    "        \"                    continue\\n\",\n",
    "        \"                Q1 = df[col].quantile(0.25)\\n\",\n",
    "        \"                Q3 = df[col].quantile(0.75)\\n\",\n",
    "        \"                IQR = Q3 - Q1\\n\",\n",
    "        \"                lower_bound = Q1 - 1.5 * IQR\\n\",\n",
    "        \"                upper_bound = Q3 + 1.5 * IQR\\n\",\n",
    "        \"                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\\n\",\n",
    "        \"                results[col] = {\\n\",\n",
    "        \"                    'outlier_count': len(outliers),\\n\",\n",
    "        \"                    'outlier_indices': outliers.index.strftime('%Y-%m-%d %H:%M:%S').tolist()[:5],\\n\",\n",
    "        \"                    'summary': f\\\"Detected {len(outliers)} outliers in {col}. Q1={Q1:.2f}, Q3={Q3:.2f}, IQR={IQR:.2f}, Bounds=[{lower_bound:.2f}, {upper_bound:.2f}].\\\"\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            logging.error(f\\\"Error in outlier detection: {e}\\\")\\n\",\n",
    "        \"            for col in CONFIG['FEATURES'] + CONFIG['TARGETS']:\\n\",\n",
    "        \"                results[col]['summary'] = f\\\"Error detecting outliers for {col}: {e}\\\"\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save results\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        file_path = os.path.join(CONFIG['RESULTS_DIR'], 'outlier_results.json')\\n\",\n",
    "        \"        with open(file_path, 'w') as f:\\n\",\n",
    "        \"            json.dump(results, f, indent=2, default=json_serializable)\\n\",\n",
    "        \"        logging.info(f\\\"Outlier results saved to {file_path}\\\")\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error saving outlier_results.json: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return results\\n\",\n",
    "        \"\\n\",\n",
    "        \"outlier_results = detect_outliers(df)\\n\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"metadata\": {},\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"# Generate EDA Questions\\n\",\n",
    "        \"def generate_eda_questions(time_results, correlation_results, lagged_results, outlier_results):\\n\",\n",
    "        \"    questions = {\\n\",\n",
    "        \"        'time_consistency': {\\n\",\n",
    "        \"            'questions': [\\n\",\n",
    "        \"                {\\n\",\n",
    "        \"                    'question': 'Are the timestamps consistent and properly spaced?',\\n\",\n",
    "        \"                    'answer': 'The dataset has {irregular_timestamps} irregular timestamps, with a median sampling frequency of {frequency_minutes} minutes, which is {consistency_status} consistent. Found {duplicate_count} duplicate timestamps{duplicate_details}.'\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"            ]\\n\",\n",
    "        \"        },\\n\",\n",
    "        \"        'temporal_trends': {\\n\",\n",
    "        \"            'questions': [\\n\",\n",
    "        \"                {\\n\",\n",
    "        \"                    'question': 'What temporal patterns exist in power consumption?',\\n\",\n",
    "        \"                    'answer': 'Hourly, daily, and weekly patterns were analyzed for each zone, showing variations in mean and standard deviation.'\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"            ]\\n\",\n",
    "        \"        },\\n\",\n",
    "        \"        'environmental_relationships': {\\n\",\n",
    "        \"            'questions': [\\n\",\n",
    "        \"                {\\n\",\n",
    "        \"                    'question': 'How do environmental features correlate with power consumption?',\\n\",\n",
    "        \"                    'answer': 'Zone 1 power consumption has a correlation of {zone_1_temp_corr} with Temperature, and Zone 2 has a correlation of {zone_2_humidity_corr} with Humidity.'\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"            ]\\n\",\n",
    "        \"        },\\n\",\n",
    "        \"        'lag_effects': {\\n\",\n",
    "        \"            'questions': [\\n\",\n",
    "        \"                {\\n\",\n",
    "        \"                    'question': 'Are there significant lag effects in power consumption?',\\n\",\n",
    "        \"                    'answer': 'For {zone}, the 1-hour lag correlation is {lag_1_temp_corr}, and the 3-hour lag correlation is {lag_3_temp_corr}.'\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"            ]\\n\",\n",
    "        \"        },\\n\",\n",
    "        \"        'data_quality': {\\n\",\n",
    "        \"            'questions': [\\n\",\n",
    "        \"                {\\n\",\n",
    "        \"                    'question': 'Are there outliers or anomalies in the data?',\\n\",\n",
    "        \"                    'answer': '{outlier_count} columns have outliers: {outlier_details}.' if len(outlier_cols) > 0 else 'No outliers detected in any columns.'\\n\",\n",
    "        \"                }\\n\",\n",
    "        \"            ]\\n\",\n",
    "        \"        }\\n\",\n",
    "        \"    }\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Add detailed outlier summaries\\n\",\n",
    "        \"    outlier_cols = [k for k, v in outlier_results.items() if k != 'boxplot' and v.get('outlier_count', 0) > 0]\\n\",\n",
    "        \"    questions['data_quality']['questions'][0]['answer'] = (\\n\",\n",
    "        \"        f\\\"{len(outlier_cols)} columns have outliers: {', '.join([f'{k}: {v['outlier_count']} outliers ({v['summary']})' for k, v in outlier_results.items() if k != 'boxplot' and v.get('outlier_count', 0) > 0])}\\\"\\n\",\n",
    "        \"        if outlier_cols else\\n\",\n",
    "        \"        \\\"No outliers detected in any columns. All features and targets were analyzed using the IQR method.\\\"\\n\",\n",
    "        \"    )\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    # Save results\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        file_path = os.path.join(CONFIG['RESULTS_DIR'], 'eda_questions.json')\\n\",\n",
    "        \"        with open(file_path, 'w') as f:\\n\",\n",
    "        \"            json.dump(questions, f, indent=2, default=json_serializable)\\n\",\n",
    "        \"        logging.info(f\\\"EDA questions saved to {file_path}\\\")\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        logging.error(f\\\"Error saving eda_questions.json: {e}\\\")\\n\",\n",
    "        \"    \\n\",\n",
    "        \"    return questions\\n\",\n",
    "        \"\\n\",\n",
    "        \"eda_questions = generate_eda_questions(time_results, correlation_results, lagged_results, outlier_results)\\n\"\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"kernelspec\": {\n",
    "      \"display_name\": \"Python 3\",\n",
    "      \"language\": \"python\",\n",
    "      \"name\": \"python3\"\n",
    "    },\n",
    "    \"language_info\": {\n",
    "      \"codemirror_mode\": {\n",
    "        \"name\": \"ipython\",\n",
    "        \"version\": 3\n",
    "      },\n",
    "      \"file_extension\": \".py\",\n",
    "      \"mimetype\": \"text/x-python\",\n",
    "      \"name\": \"python\",\n",
    "      \"nbconvert_exporter\": \"python\",\n",
    "      \"pygments_lexer\": \"ipython3\",\n",
    "      \"version\": \"3.8.10\"\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
